{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmicsML/dance-tutorials/blob/main/dance_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installation\n",
        "\n",
        "DANCE is published on [PyPI](https://pypi.org/project/pydance/). Thus, installing DANCE is as easy as\n",
        "\n",
        "```bash\n",
        "pip install pydance\n",
        "```\n",
        "\n",
        "Or, to install the latest dev version on GitHub as\n",
        "\n",
        "```bash\n",
        "pip install git+https://github.com/OmicsML/dance\n",
        "```\n",
        "\n",
        "But becaues DANCE includes many deep learning based methods, there are also deep learning library dependencies, such as [PyTorch](https://pytorch.org/), [PyG](https://www.pyg.org/), and [DGL](https://www.dgl.ai/). We will walk through the installation process below."
      ],
      "metadata": {
        "id": "ugihXw2wU92o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Install torch related dependencies"
      ],
      "metadata": {
        "id": "a9uCJIzekYW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab comes with torch installed, so we do not need to install pytorch here\n",
        "# !pip3 install torch torchvision torchaudio\n",
        "\n",
        "!pip install -q torch_geometric==2.3.1\n",
        "!pip install -q dgl==1.1.0 -f https://data.dgl.ai/wheels/cu117/repo.html\n",
        "!pip install -q torchnmf==0.3.4"
      ],
      "metadata": {
        "id": "-9vkgggHU5Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Install DANCE v1.0.0"
      ],
      "metadata": {
        "id": "t0Ai6ohLkpyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydance==1.0.0"
      ],
      "metadata": {
        "id": "xiNfZpuOkewF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Check if DANCE is installed successfully"
      ],
      "metadata": {
        "id": "dPyaIcBPk4FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dance\n",
        "print(f\"Installed DANCE version {dance.__version__}\")"
      ],
      "metadata": {
        "id": "Dwap0oVmk1hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data loading and processing\n",
        "\n",
        "DANCE comes with several benchmarking datasets in a unified dataset object format. This makes data downloading, processing, and caching easy for users through our dataset object interface."
      ],
      "metadata": {
        "id": "qpq3fr_7mkz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Check available data options and load data object"
      ],
      "metadata": {
        "id": "-hPDdWsPpa5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "from pprint import pprint\n",
        "from dance.datasets.singlemodality import ClusteringDataset, ScDeepSortDataset"
      ],
      "metadata": {
        "id": "3oylxVi7oA7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available dataset option for ClusteringDataset:\")\n",
        "pprint(ClusteringDataset.get_avalilable_data())"
      ],
      "metadata": {
        "id": "geQmHab8pnsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available dataset option for ScDeepSortDataset:\")\n",
        "pprint(ScDeepSortDataset.get_avalilable_data())"
      ],
      "metadata": {
        "id": "_3p6UfZHCOES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: ClusteringDataset"
      ],
      "metadata": {
        "id": "1WcWc4dUCSC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ClusteringDataset(\"10X_PBMC\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "wtg5pNZgCdac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset object do not contain data, it only loads the data upon calling\n",
        "# the load_data function\n",
        "data = dataset.load_data()"
      ],
      "metadata": {
        "id": "EhiCX5BpooNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: ScDeepSortDataset"
      ],
      "metadata": {
        "id": "J0b-GAFnCxRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ScDeepSortDataset(species=\"mouse\", tissue=\"Brain\",\n",
        "                            train_dataset=[\"3285\", \"753\"], test_dataset=[\"2695\"])\n",
        "data = dataset.load_data()"
      ],
      "metadata": {
        "id": "00_UOrqlrA_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. A quick primer on AnnData\n",
        "\n",
        "<img\n",
        "  src=\"https://raw.githubusercontent.com/scverse/anndata/main/docs/_static/img/anndata_schema.svg\"\n",
        "  align=\"right\" width=\"450\" alt=\"image\"\n",
        "/>\n",
        "\n",
        "The [dance data object](https://github.com/OmicsML/dance/blob/912405cb5ab43caf16eb22b9216865c7e3976eaf/dance/data/base.py#L40) is heavily built on top of [AnnData](https://anndata.readthedocs.io/en/latest/), which is a widely used data object to represent, store, and manipulate large annotated matrices.\n",
        "\n",
        "> anndata is a Python package for handling annotated data matrices in memory and on disk, positioned between pandas and xarray. anndata offers a broad range of computationally efficient features...\n",
        "\n",
        "AnnData falls into the ecosystem of scVerse, providing extra advantage and ease for handeling single-cell data using, for example, [Scanpy](https://scanpy.readthedocs.io/en/stable/)."
      ],
      "metadata": {
        "id": "cU0EHCYiLzou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dance data object essentially wraps around an AnnData object,\n",
        "which can be accessed in the `.data` attribute."
      ],
      "metadata": {
        "id": "oLrtDujjC-O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adata = data.data\n",
        "print(adata)"
      ],
      "metadata": {
        "id": "glGCQdF0Lx5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cells, num_genes = adata.shape\n",
        "print(f\"There are {num_cells:,} cells and {num_genes:,} genes in this data object.\")"
      ],
      "metadata": {
        "id": "bsBaT1jqEkDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several key attributes in AnnData objects. For example, `.X` typically holds the main data, such as gene expression. `obs` and `obsm` hold metadata for each sample (i.e., a cell)."
      ],
      "metadata": {
        "id": "107dAHfTDOOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adata.X"
      ],
      "metadata": {
        "id": "yJYYK4ZFLxtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata.obsm[\"cell_type\"]"
      ],
      "metadata": {
        "id": "O5R2Rh_hLxim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Data pre-processing using transforms\n",
        "\n",
        "Applying individual in-place transformations to data\n"
      ],
      "metadata": {
        "id": "0ytH_-HIrNHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc\n",
        "from dance.transforms import AnnDataTransform, FilterGenesPercentile"
      ],
      "metadata": {
        "id": "AYlMut_m02Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Library sizes before normalization: {data.data.X.sum(1).round(0)}\")\n",
        "\n",
        "# Library size normalization\n",
        "AnnDataTransform(sc.pp.normalize_total, target_sum=1e4)(data)\n",
        "\n",
        "print(f\"Library sizes after normalization: {data.data.X.sum(1).round(0)}\")"
      ],
      "metadata": {
        "id": "FhfaqIJP09pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shifted log transformation\n",
        "AnnDataTransform(sc.pp.log1p)(data)\n",
        "\n",
        "print(f\"Sum of expression per cell after log1p transformation: {data.data.X.sum(1)}\")"
      ],
      "metadata": {
        "id": "DZ9nEPA21u55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of genes before filtering: {data.shape[1]:,}\")\n",
        "\n",
        "# Filter out genes that have extreme coefficient of variation\n",
        "FilterGenesPercentile(min_val=1, max_val=99, mode=\"sum\")(data)\n",
        "\n",
        "print(f\"Number of genes before filtering: {data.shape[1]:,}\")"
      ],
      "metadata": {
        "id": "EGOxkYn00-ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Composing transformations into a a pre-precoessing pipeline (feat. caching)"
      ],
      "metadata": {
        "id": "2St8ISsAr1_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dance.transforms import Compose\n",
        "\n",
        "preprocessing_pipeline = Compose(\n",
        "    AnnDataTransform(sc.pp.normalize_total, target_sum=1e-4),\n",
        "    AnnDataTransform(sc.pp.log1p),\n",
        "    FilterGenesPercentile(min_val=1, max_val=99, mode=\"sum\"),\n",
        ")\n",
        "\n",
        "# Now we can apply the preprocessing pipeline transformation to our data\n",
        "# data = dataset.load_data()\n",
        "# preprocessing_pipeline(data)\n",
        "\n",
        "# Alternatively, we can also pass the transformation to the loading function\n",
        "data = dataset.load_data(transform=preprocessing_pipeline, cache=True)"
      ],
      "metadata": {
        "id": "NEhY1hTRo_nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reloading the data with cache enabled using the same transformation\n",
        "# before can significantly reduce the data loading and pre-processing\n",
        "# time. Making it easier for researcher to run evaluation with different\n",
        "# configurations many times but with the same pre-processed data\n",
        "data = dataset.load_data(transform=preprocessing_pipeline, cache=True)"
      ],
      "metadata": {
        "id": "9L8DaBvnuRLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Single modality tasks"
      ],
      "metadata": {
        "id": "mkvsZ3K7YSV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Example: ACTINN for Cell Type Annotation\n",
        "\n",
        "#### Model structure\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/singlemodality/mlp_visualization.png)\n",
        "\n",
        "#### Visualization of annotation results\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/singlemodality/cell_type_visualization.png)"
      ],
      "metadata": {
        "id": "YP6RTEfKHAli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data"
      ],
      "metadata": {
        "id": "z-eaoyhCGuTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available dataset option for ScDeepSortDataset:\")\n",
        "pprint(ScDeepSortDataset.get_avalilable_data())"
      ],
      "metadata": {
        "id": "bA7PPc2wvM3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from dance.modules.single_modality.cell_type_annotation.actinn import ACTINN\n",
        "from dance.utils import set_seed\n",
        "\n",
        "# Initialize model and get model specific preprocessing pipeline\n",
        "model = ACTINN(hidden_dims=[256, 256], lambd=0.01, device='cuda')\n",
        "preprocessing_pipeline = model.preprocessing_pipeline(normalize=True, filter_genes=True)\n",
        "\n",
        "# Load data and perform necessary preprocessing\n",
        "dataset = ScDeepSortDataset(species=\"mouse\", tissue=\"Brain\",\n",
        "                            train_dataset=[\"3285\", \"753\"], test_dataset=[\"2695\"])\n",
        "data = dataset.load_data(transform=preprocessing_pipeline, cache=True)"
      ],
      "metadata": {
        "id": "wzZBJS2pvM0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate model"
      ],
      "metadata": {
        "id": "eNKJg6sYG0Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain training and testing data\n",
        "x_train, y_train = data.get_train_data(return_type=\"torch\")\n",
        "x_test, y_test = data.get_test_data(return_type=\"torch\")"
      ],
      "metadata": {
        "id": "QW4EgXbwvSKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)"
      ],
      "metadata": {
        "id": "6hJhRnGOvSHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "id": "ruA5vKQ-vSEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate model\n",
        "set_seed(42)\n",
        "model.fit(x_train, y_train, lr=0.001, num_epochs=21,\n",
        "          batch_size=1000, print_cost=True)\n",
        "print(f\"ACC: {model.score(x_test, y_test):.4f}\")"
      ],
      "metadata": {
        "id": "VZjdWCOwvVJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.model)"
      ],
      "metadata": {
        "id": "qevK79covVMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Example: GraphSCI for Imputation\n",
        "\n",
        "#### Model structure\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/singlemodality/graphsci_visualization.png)\n",
        "\n",
        "#### Reported results\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/singlemodality/imputation_results_example.png)"
      ],
      "metadata": {
        "id": "APzGBDTSG7e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data"
      ],
      "metadata": {
        "id": "MDO_0RIFHozy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dance.datasets.singlemodality import ImputationDataset\n",
        "from dance.modules.single_modality.imputation.graphsci import GraphSCI\n",
        "from dance.utils import set_seed\n",
        "\n",
        "# Load data and perform preprocessing\n",
        "set_seed(42)\n",
        "dataloader = ImputationDataset(data_dir='./data', dataset='pbmc_data', train_size=0.9)\n",
        "preprocessing_pipeline = GraphSCI.preprocessing_pipeline(mask=True, mask_rate=0.1)\n",
        "data = dataloader.load_data(transform=preprocessing_pipeline, cache=True)"
      ],
      "metadata": {
        "id": "yxXzPNjSG6jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.data.layers['train_mask']"
      ],
      "metadata": {
        "id": "pTwtpDNcvdYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.data.layers['valid_mask']"
      ],
      "metadata": {
        "id": "Drw8p9_EvdcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate model"
      ],
      "metadata": {
        "id": "R78kh2NvHr7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain training and testing data\n",
        "X, X_raw, g, mask = data.get_x(return_type=\"default\")\n",
        "device = 'cuda:0'\n",
        "X = torch.tensor(X.toarray()).to(device)\n",
        "X_raw = torch.tensor(X_raw.toarray()).to(device)\n",
        "g = g.to(device)\n",
        "train_idx = data.train_idx\n",
        "test_idx = data.test_idx\n",
        "\n",
        "# Train and evaluate model\n",
        "model = GraphSCI(num_cells=X.shape[0], num_genes=X.shape[1],\n",
        "                 dataset='pbmc_data', gpu=0)\n",
        "model.fit(X, X_raw, g, train_idx, mask, n_epochs=10, la=1e-7)\n",
        "model.load_model()\n",
        "imputed_data = model.predict(X, X_raw, g, mask)\n",
        "score = model.score(X_raw, imputed_data, test_idx, mask, metric='RMSE')\n",
        "print(\"RMSE: %.4f\" % score)"
      ],
      "metadata": {
        "id": "jbo4IZIMG7R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Example: scDeepCluster for Clustering\n",
        "\n",
        "#### Model structure\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/singlemodality/scdeepcluster_visualization.png)\n",
        "\n",
        "#### Reported results\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/singlemodality/clustering_results_example.png)"
      ],
      "metadata": {
        "id": "-0QOxl1iHvdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data"
      ],
      "metadata": {
        "id": "7F-WvpQ9H_Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dance.datasets.singlemodality import ClusteringDataset\n",
        "from dance.modules.single_modality.clustering.scdeepcluster import ScDeepCluster\n",
        "from dance.utils import set_seed\n",
        "\n",
        "\n",
        "# Load data and perform necessary preprocessing\n",
        "dataloader = ClusteringDataset('./data', '10X_PBMC')\n",
        "preprocessing_pipeline = ScDeepCluster.preprocessing_pipeline()\n",
        "data = dataloader.load_data(transform=preprocessing_pipeline)"
      ],
      "metadata": {
        "id": "dUF8K64tHv9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate model"
      ],
      "metadata": {
        "id": "Qo6CQAeaICio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs: x, x_raw, n_counts\n",
        "inputs, y = data.get_train_data()\n",
        "n_clusters = len(np.unique(y))\n",
        "in_dim = inputs[0].shape[1]\n",
        "\n",
        "# Build and train model\n",
        "set_seed(42)\n",
        "model = ScDeepCluster(input_dim=in_dim, z_dim=32, encodeLayer=[256, 64], decodeLayer=[64, 256], device='cuda')\n",
        "model.fit(inputs, y, n_clusters=n_clusters, lr=0.01, epochs=3, pt_epochs=3)\n",
        "\n",
        "# Evaluate model predictions\n",
        "score = model.score(None, y)\n",
        "print(f\"ARI: {score:.4f}\")"
      ],
      "metadata": {
        "id": "74Pkn2ztHwRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Multi-modality tasks"
      ],
      "metadata": {
        "id": "O2CSLVVBIT01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Modality Prediction"
      ],
      "metadata": {
        "id": "ilgPs_VMIa5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task and Model Description\n",
        "\n",
        "Modality Prediction: predicting the flow of information from DNA to RNA and RNA to Protein.\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/multimodality/modality_prediction_visualization.svg)\n",
        "\n",
        "In this section, we take RNA-to-Protein as an example task, where the data are obtained from CITE-seq technology. We use BABEL[1] model as an example to demonstrate the workflow of DANCE package.\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/multimodality/babel_visualization.jpeg)\n",
        "\n",
        "[1] Wu, Kevin E., et al. \"BABEL enables cross-modality translation between multiomic profiles at single-cell resolution.\" Proceedings of the National Academy of Sciences 118.15 (2021): e2023070118."
      ],
      "metadata": {
        "id": "tWUwYvvrJhLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import packages and initializations"
      ],
      "metadata": {
        "id": "G_lWTzJUJuOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "\n",
        "import anndata\n",
        "import mudata\n",
        "import scanpy as sc\n",
        "import torch\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from dance import logger\n",
        "from dance.data import Data\n",
        "from dance.datasets.multimodality import ModalityPredictionDataset\n",
        "from dance.modules.multi_modality.predict_modality.babel import BabelWrapper\n",
        "from dance.utils import set_seed\n",
        "\n",
        "set_seed(42)\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "OUWrqab2ITLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data and perform necessary preprocessing"
      ],
      "metadata": {
        "id": "p1e1yhinJ0Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ModalityPredictionDataset(\"openproblems_bmmc_cite_phase2_rna_subset\")\n",
        "data = dataset.load_data()"
      ],
      "metadata": {
        "id": "6rY-HCL2IZr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct data object\n",
        "data.set_config(feature_mod=\"mod1\", label_mod=\"mod2\")\n",
        "\n",
        "# Obtain training and testing data\n",
        "x_train, y_train = data.get_train_data(return_type=\"torch\")\n",
        "x_test, y_test = data.get_test_data(return_type=\"torch\")"
      ],
      "metadata": {
        "id": "12b8zfD3IZ-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "LWEhJDOBv43v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Specify hyperparameters and initialize the model"
      ],
      "metadata": {
        "id": "_64BDx2Wv_KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "######## Important hyperparameters\n",
        "parser.add_argument(\"--subtask\", default=\"openproblems_bmmc_cite_phase2_rna_subset\")\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=40)\n",
        "parser.add_argument(\"--lr\", \"-l\", type=float, default=0.01, help=\"Learning rate\")\n",
        "parser.add_argument(\"--batchsize\", \"-b\", type=int, default=64, help=\"Batch size\")\n",
        "parser.add_argument(\"--hidden\", type=int, default=64, help=\"Hidden dimensions\")\n",
        "parser.add_argument(\"--earlystop\", type=int, default=2, help=\"Early stopping after N epochs\")\n",
        "parser.add_argument(\"--naive\", \"-n\", action=\"store_true\", help=\"Use a naive model instead of lego model\")\n",
        "parser.add_argument(\"--lossweight\", type=float, default=1., help=\"Relative loss weight\")\n",
        "########\n",
        "\n",
        "parser.add_argument(\"--model_folder\", default=\"./\")\n",
        "parser.add_argument(\"--outdir\", \"-o\", default=\"./\", help=\"Directory to output to\")\n",
        "parser.add_argument(\"--resume\", action=\"store_true\")\n",
        "parser.add_argument(\"--device\", default=\"cuda\")\n",
        "parser.add_argument(\"--cpus\", default=1, type=int)\n",
        "parser.add_argument(\"--rnd_seed\", default=42, type=int)\n",
        "\n",
        "args_defaults = parser.parse_args([])\n",
        "args = argparse.Namespace(**vars(args_defaults))\n",
        "args"
      ],
      "metadata": {
        "id": "g3DZ7fEqv-21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BabelWrapper(args, dim_in=x_train.shape[1], dim_out=y_train.shape[1])"
      ],
      "metadata": {
        "id": "Ec-fO2V0J6rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate model"
      ],
      "metadata": {
        "id": "XXaT400QJ9or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train.float(), y_train.float(), val_ratio=0.15)"
      ],
      "metadata": {
        "id": "_pSaYI7aJ6o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x_test.float())"
      ],
      "metadata": {
        "id": "yCH4urZsJ6bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test.float(), y_test.float())"
      ],
      "metadata": {
        "id": "Zqy4zqfQIaAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Modality Matching"
      ],
      "metadata": {
        "id": "z29G7PPcGtjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching profiles of each cell from different modalities.\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/multimodality/modality_matching_visualization.jpeg)\n",
        "\n",
        "In this section, we take RNA-to-Protein as an example task, where the data are obtained from CITE-seq technology. We use scMoGNN[2] model as an example to demonstrate the workflow of DANCE package.\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/multimodality/scmogcn_visualization.jpeg)\n",
        "\n",
        "[2] Wen, Hongzhi, et al. \"Graph neural networks for multimodal single-cell data integration.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022."
      ],
      "metadata": {
        "id": "Y5IWp0PHG_xT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data and perform necessary preprocessing"
      ],
      "metadata": {
        "id": "ooQd3gmnHIN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dance.datasets.multimodality import ModalityMatchingDataset\n",
        "from dance.modules.multi_modality.match_modality.scmogcn import ScMoGCNWrapper\n",
        "from dance.transforms.graph.cell_feature_graph import CellFeatureBipartiteGraph\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "dataset = ModalityMatchingDataset('openproblems_bmmc_cite_phase2_rna_subset', root='./data', preprocess=\"pca\", pkl_path='lsi_input_pca_count.pkl')\n",
        "data = dataset.load_data()"
      ],
      "metadata": {
        "id": "M8rAJxqLGsTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ScMoGNN graph construction\n",
        "data = CellFeatureBipartiteGraph(cell_feature_channel=\"X_pca\", mod=\"mod1\")(data)\n",
        "data = CellFeatureBipartiteGraph(cell_feature_channel=\"X_pca\", mod=\"mod2\")(data)\n",
        "data.set_config(feature_mod=[\"mod1\", \"mod2\", \"mod1\", \"mod2\"], feature_channel_type=[\"uns\", \"uns\", \"obs\", \"obs\"],\n",
        "                feature_channel=[\"g\", \"g\", \"batch\", \"batch\"], label_mod=\"mod1\", label_channel=\"labels\")"
      ],
      "metadata": {
        "id": "14mocJh3Gsm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(g_mod1, g_mod2, batch_mod1, batch_mod2), z = data.get_data(return_type=\"default\")\n",
        "train_size = len(data.get_split_idx(\"train\"))\n",
        "test_idx = np.arange(train_size, g_mod1.num_nodes(\"cell\"))\n",
        "z_test = F.one_hot(torch.from_numpy(z[train_size:]).long())\n",
        "labels1 = torch.argmax(z_test, dim=0).to(device)\n",
        "labels2 = torch.argmax(z_test, dim=1).to(device)\n",
        "g_mod1 = g_mod1.to(device)\n",
        "g_mod2 = g_mod2.to(device)"
      ],
      "metadata": {
        "id": "muMV5snDGsp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Specify hyperparametsr and initialize the model"
      ],
      "metadata": {
        "id": "a2GZz64qHT-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--layers\", default=4, type=int, choices=[3, 4, 5, 6, 7])\n",
        "parser.add_argument(\"--learning_rate\", default=6e-4, type=float)\n",
        "parser.add_argument(\"--disable_propagation\", default=0, type=int, choices=[0, 1, 2])\n",
        "parser.add_argument(\"--auxiliary_loss\", default=True, type=bool)\n",
        "parser.add_argument(\"--epochs\", default=2000, type=int)\n",
        "parser.add_argument(\"--hidden_size\", default=64, type=int)\n",
        "parser.add_argument(\"--temperature\", default=2.739896, type=float)\n",
        "parser.add_argument(\"--device\", default='cuda', type=str)\n",
        "parser.add_argument(\"--rnd_seed\", default=42, type=int)\n",
        "\n",
        "args_defaults = parser.parse_args([])\n",
        "args = argparse.Namespace(**vars(args_defaults))\n",
        "data_folder = './data/'\n",
        "device = 'cuda'\n",
        "args"
      ],
      "metadata": {
        "id": "jGmectIGGssz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ScMoGCNWrapper(\n",
        "    args,\n",
        "    [\n",
        "        [(g_mod1.num_nodes(\"feature\"), 512, 0.25), (512, 512, 0.25), (512, args.hidden_size)],\n",
        "        [(g_mod2.num_nodes(\"feature\"), 512, 0.2), (512, 512, 0.2), (512, args.hidden_size)],\n",
        "        [(args.hidden_size, 512, 0.2), (512, g_mod1.num_nodes(\"feature\"))],\n",
        "        [(args.hidden_size, 512, 0.2), (512, g_mod2.num_nodes(\"feature\"))],\n",
        "    ],\n",
        "    args.temperature,\n",
        ")"
      ],
      "metadata": {
        "id": "f5plMkDfGsvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate model"
      ],
      "metadata": {
        "id": "_ueVlu-6HYV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(g_mod1, g_mod2, labels1, labels2, train_size=train_size)"
      ],
      "metadata": {
        "id": "BJ9K9YCfGsx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(test_idx, enhance=True, batch1=batch_mod1, batch2=batch_mod2)"
      ],
      "metadata": {
        "id": "PxDuWlKoGs0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(test_idx, labels_matrix=z_test, enhance=True, batch1=batch_mod1, batch2=batch_mod2)"
      ],
      "metadata": {
        "id": "tXwY3LVFGs3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Spatial tasks"
      ],
      "metadata": {
        "id": "Y3Y2XVUyKD40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Spatial Domain"
      ],
      "metadata": {
        "id": "FZS7TIBOKQ3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SpaGCN model for spatial domain identification\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/spatial/spagcn_framework.png)"
      ],
      "metadata": {
        "id": "D0PPLfYdKZNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dance.transforms import Compose\n",
        "from dance.datasets.spatial import SpatialLIBDDataset\n",
        "from dance.modules.spatial.spatial_domain.spagcn import SpaGCN\n",
        "from dance.utils import set_seed\n",
        "from dance.transforms import CellPCA, Compose, FilterGenesMatch, SetConfig\n",
        "from dance.transforms.graph import SpaGCNGraph, SpaGCNGraph2D"
      ],
      "metadata": {
        "id": "gbLBaY5BwRCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize model and get model specific preprocessing *pipeline*"
      ],
      "metadata": {
        "id": "gC8_MNm9Knjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpaGCN()\n",
        "# In SpaGCN, alpha and beta are used for graph construction\n",
        "preprocessing_pipeline = model.preprocessing_pipeline(alpha=1, beta=49)"
      ],
      "metadata": {
        "id": "Vtl12VGvKQaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User defined customized transform"
      ],
      "metadata": {
        "id": "MCENWWqAKrEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_pipeline = Compose(\n",
        "    FilterGenesMatch(prefixes=[\"ERCC\", \"MT-\"]),\n",
        "    SpaGCNGraph(alpha=1, beta=49),\n",
        "    SpaGCNGraph2D(),\n",
        "    CellPCA(n_components=40),\n",
        "    SetConfig({\n",
        "        \"feature_channel\": [\"CellPCA\", \"SpaGCNGraph\", \"SpaGCNGraph2D\"],\n",
        "        \"feature_channel_type\": [\"obsm\", \"obsp\", \"obsp\"],\n",
        "        \"label_channel\": \"label\",\n",
        "        \"label_channel_type\": \"obs\"\n",
        "    }),\n",
        ")"
      ],
      "metadata": {
        "id": "j0EXszkKKQXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data and perform necessary preprocessing"
      ],
      "metadata": {
        "id": "8m8cYF5VKwsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = SpatialLIBDDataset(data_id=\"151673\")\n",
        "data = dataloader.load_data(transform=preprocessing_pipeline, cache=\"store_true\")"
      ],
      "metadata": {
        "id": "616ANzlKwaks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "hfPuJmV0wcmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.data.obsp[\"SpaGCNGraph\"].shape"
      ],
      "metadata": {
        "id": "-ygJh7-TK1Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.x[0].shape"
      ],
      "metadata": {
        "id": "F8Nnkh_NK1A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.x[1].shape"
      ],
      "metadata": {
        "id": "O-_Sj-smK09p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x, adj, adj_2d), y = data.get_train_data()"
      ],
      "metadata": {
        "id": "IYTjlVHswiNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, x.shape"
      ],
      "metadata": {
        "id": "VsHgLCwtwiUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj, adj.shape"
      ],
      "metadata": {
        "id": "Om03IJebwie6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y, y.shape"
      ],
      "metadata": {
        "id": "JvR4xO-8wl8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate model"
      ],
      "metadata": {
        "id": "mGUEFoXLK57h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = model.search_l(0.05, adj, start=0.01, end=1000, tol=5e-3, max_run=200)\n",
        "model.set_l(l)\n",
        "res = model.search_set_res((x, adj), l=l, target_num=7, start=0.4, step=0.1,\n",
        "                           tol=5e-3, lr=0.05, epochs=200, max_run=200)"
      ],
      "metadata": {
        "id": "BJLyTTQyK06z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.fit_predict((x, adj), init_spa=True, init=\"louvain\", tol=5e-3,\n",
        "                         lr=0.05, epochs=200, res=res)"
      ],
      "metadata": {
        "id": "c1fTJ2-vK041"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.default_score_func(y, pred)\n",
        "print(f\"ARI: {score:.4f}\")"
      ],
      "metadata": {
        "id": "bD4PmvfZK04E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Cell Type Deconvolution"
      ],
      "metadata": {
        "id": "9tEEPzblLLf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DSTG model for cell type deconvolution\n",
        "\n",
        "![image](https://github.com/OmicsML/dance-tutorials/raw/main/imgs/tutorial_v1/spatial/dstg_framework.png)"
      ],
      "metadata": {
        "id": "uWeBYQgFLPgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from dance.datasets.spatial import CellTypeDeconvoDataset\n",
        "from dance.modules.spatial.cell_type_deconvo import DSTG\n",
        "from dance.utils import set_seed"
      ],
      "metadata": {
        "id": "JuUBSEx0LWvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get model specific preprocessing *pipeline*"
      ],
      "metadata": {
        "id": "T0Huz17oLbxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_pipeline = DSTG.preprocessing_pipeline(\n",
        "    n_pseudo=500,\n",
        "    n_top_genes=2000,\n",
        "    k_filter=200,\n",
        "    num_cc=30,\n",
        ")"
      ],
      "metadata": {
        "id": "SNewOFwMLcws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data and perform necessary preprocessing"
      ],
      "metadata": {
        "id": "WUuvU7AsLhu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CellTypeDeconvoDataset(data_dir=\"data/spatial\", data_id=\"CARD_synthetic\")\n",
        "data = dataset.load_data(transform=preprocessing_pipeline, cache=\"store_true\")"
      ],
      "metadata": {
        "id": "M-z7K16wLcPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.x"
      ],
      "metadata": {
        "id": "6bzAub6oK0xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data.x)"
      ],
      "metadata": {
        "id": "lBbjqjkGLnLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.x[0], data.x[0].shape"
      ],
      "metadata": {
        "id": "Y01dDri6LnHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.x[1], data.x[1].shape"
      ],
      "metadata": {
        "id": "kEyieaeILnEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(adj, x), y = data.get_data(return_type=\"default\")\n",
        "x, y = torch.FloatTensor(x), torch.FloatTensor(y.values)\n",
        "adj = torch.sparse.FloatTensor(torch.LongTensor([adj.row.tolist(), adj.col.tolist()]),\n",
        "                               torch.FloatTensor(adj.data.astype(np.int32)))"
      ],
      "metadata": {
        "id": "IyZ1rXFkLnBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, x.shape"
      ],
      "metadata": {
        "id": "nQdWqipvxX_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj, adj.shape"
      ],
      "metadata": {
        "id": "Fp_iEPSPxYCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y, y.shape"
      ],
      "metadata": {
        "id": "0Pss6LONxYFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask = data.get_split_mask(\"pseudo\", return_type=\"torch\")\n",
        "inputs = (adj, x, train_mask)\n",
        "train_mask, train_mask.shape"
      ],
      "metadata": {
        "id": "tq0ZmCStxYHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate model"
      ],
      "metadata": {
        "id": "8K4uFzomLrIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DSTG(nhid=16, bias=False, dropout=0, device=\"auto\")\n",
        "pred = model.fit_predict(inputs, y, lr=0.01, max_epochs=25, weight_decay=0.0001)\n",
        "pred, pred.shape"
      ],
      "metadata": {
        "id": "yR-8s8HgxfJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mask = data.get_split_mask(\"test\", return_type=\"torch\")\n",
        "test_mask, test_mask.shape"
      ],
      "metadata": {
        "id": "YPVLCbkXxewS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.default_score_func(y[test_mask], pred[test_mask])\n",
        "print(f\"MSE: {score:7.4f}\")"
      ],
      "metadata": {
        "id": "8GgGkurzLm-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}